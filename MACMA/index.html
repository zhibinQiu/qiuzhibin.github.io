<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">

    <title>MACMA</title>
  </head>
  <body>
  	<div style="text-align:center;">
		<div id="intro" style="width: 100%; padding:40px; text-align:center;">
	        <h1>Measuring Acoustics with Collaborative Multiple Agents</h1>
        </div>
		<div class="container">
		  <div class="row">
		    <div class="col-sm">
		      <a href="https://yyf17.github.io/"><h4> Yinfeng Yu </h4></a>    
		    </div>
		    <div class="col-sm">
		      <a href="https://changan.io/"><h4> Changan Chen </h4></a> 
            </div>
		    <div class="col-sm">
		      <a href=""><h4> Lele Cao</h4></a> 
		    </div>
		    <div class="col-sm">
                <a href=""><h4> Fangkai Yang </h4></a> 
            </div>
		    <div class="col-sm">
		      <a href=""><h4> Fuchun Sun</h4></a> 
		    </div>
		  </div>
		  <p></p>
		  <div class="row">
		  	<div class="col-lg">
		   	<h4>  Tsinghua University (THU) </h4>
		    </div>
		  </div>
		  <div class="row">
		  	<div class="col-lg">
		   	<h4> International Joint Conference on Artificial Intelligence (IJCAI), 2023 </h4>
		    </div>
		  </div>
		  
		  <div class="row">
		  	<div class="col-lg">
		    	<center><h2><strong>
					<a href="">Paper</a> |
					<a href="https://github.com/yyf17/MACMA/tree/main">Code</a> |
					<a href="./files/">Slides</a> |
				        <a href="./files/bib.txt">Bibtex</a> |
					<a href="./files/MACMA.pdf">Paper(main+appendix)</a> 
				</strong> </h2></center> 
			</div>
		  </div>
		  <br>
		  
		  <div class="row">
		  	<div class="col-lg">
		  		<img src="./files/MACMA.png" class="img-fluid" style="height:215px;">
		  	</div>
		  </div>

		  <br><br>
		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Abstract </h2>
		  	</div>
		  </div>
		  <div class="row">
		  	<div style="font-size:16px"><p align="justify">
				As humans, we hear sound every second of our life. The sound we hear is affected mainly by the acoustics of the environment surrounding us. For example, a spacious hall leads to more reverberation. Room Impulse Responses (RIR) are commonly used to characterize environment acoustics as a function of the scene geometry, materials, and source/receiver locations. Traditionally, RIRs are measured by setting up a loudspeaker and microphone in the environment for each pair of source/receiver locations, which is time-consuming and inefficient. We propose to let two robots measure the environment's acoustics by actively moving and emitting/receiving environmental signals. We also design a collaborative multi-agent policy where these two robots are trained to explore the environment's acoustics while being rewarded for accurate and widely explored predictions. We show that the robots learn to collaborate and move to explore environment acoustics while minimizing the prediction error. To our best knowledge, this is the first task where robots learn to measure environmental acoustics instead of humans.
				Project:  <a href="https://yyf17.github.io/MACMA">https://yyf17.github.io/MACMA</a>.
		  	</p>
		  	</div>
		  </div>

		  <br><br>
		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Materials </h2><p></p>
		  	</div>
		  </div>
		  <div class="row">

		    <div class="col-lg">
		    	<a href="">
          			<img src="./files/common.jpeg" style="height:270px" class="img-fluid" alt="Responsive image">
          		</a>
          	  <br>
		      <a href=""><button type="button" class="btn btn-secondary">Paper</button></a>           
		    </div>

		    <div class="col-lg">
		      	<a href="./files/">
          			<img src="files/common.jpeg" style="height:270px" class="img-fluid" alt="Responsive image">
          		</a>
          	  	<br>
		      	<a href="./files/common.jpeg"><button type="button" class="btn btn-secondary">Poster</button></a>   
		    </div>
		    

		   	<div class="col-lg">
		      <a href="https://github.com/yyf17/MACMA">
		    		<img src="files/MACMACode.png" style="height:270px" class="img-fluid" alt="Responsive image">
		    	</a>
		      <br>
		     <!--  code:   https://github.com/yyf17/MACMA  -->
		      <a href="https://github.com/yyf17/MACMA"><button type="button" class="btn btn-secondary">Code</button></a>  
		    </div> 

		  </div> 

		  <br><br>
		  <div class="row">
		  	<div class="col-lg">
		  		<h2>Presentation</h2>
			    <div class=text-center>
				   	<iframe width="560" height="315" src="./files/" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>      	
	                </iframe>
				    
				</div>
		  	</div>
		  </div>

          <br><br>
		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Citation </h2>
		  		    <pre align="left">
					@inproceedings{YinfengIJCAI2023MACMA,
					    author    = {
						Yinfeng Yu and
						Changan Chen and
						Lele Cao and 
						Fangkai Yang and
						Wenbing Huang and
						Fuchun Sun
					    },
					    title     = {Measuring Acoustics with Collaborative Multiple Agents},
					    booktitle = {The 32nd International Joint Conference on Artificial Intelligence, {IJCAI}
					    2023, Macao, 19th-25th August 2023},
					    year      = {2023}
					}
                    		</pre>
			</div>
		  </div>

		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Acknowledgement </h2>
		  		<p align="left">
				We thanks the insightful comments from the reviewers of IJCAI 2023. 
				The following projects jointly supported this work:
				the Sino-German Collaborative Research Project {\it Crossmodal Learning} with identification number \verb|NSFC62061136001/DFG SFB/TRR169|;
				the National Natural Science Foundation of China (No.61961039);
				the National Key R\&D Program of China (No.2022ZD0115803);
				the Xinjiang Natural Science Foundation (No.2022D01C432, No.2022D01C58, No.2020D01C026 and No.2015211C288);
				this work is also partially funded by THU-Bosch JCML center.
                          <br><br>
		  	</div>
		  </div>		  

		</div>
	</div>



	<div id="intro" style="width: 100%; padding:50px; text-align:center;">
	        <h1></h1>
	</div>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>
   
  </body>
</html>
