<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">

    <title>SAAVN</title>
  </head>
  <body>
  	<div style="text-align:center;">
		<div id="intro" style="width: 100%; padding:40px; text-align:center;">
	        <h1>Sound Adversarial Audio-Visual Navigation</h1>
        </div>
		<div class="container">
		  <div class="row">
		    <div class="col-sm">
		      <a href="https://yyf17.github.io/"><h4> Yinfeng Yu </h4></a>    
		    </div>
		    <div class="col-sm">
		      <a href=""><h4> Wenbing Huang</h4></a> 
		    </div>
		    <div class="col-sm">
		      <a href=""><h4> Fuchun Sun</h4></a> 
		    </div>
		    <div class="col-sm">
		      <a href="https://changan.io/"><h4> Changan Chen </h4></a> 
            </div>
            <div class="col-sm">
                <a href=""><h4> Yikai Wang </h4></a> 
            </div>
            <div class="col-sm">
                <a href=""><h4> Xiaohong Liu </h4></a> 
            </div>
		  </div>
		  <p></p>
		  <div class="row">
		  	<div class="col-lg">
		   	<h4>  Tsinghua University (THU) </h4>
		    </div>
		  </div>
		  <div class="row">
		  	<div class="col-lg">
		   	<h4> International Conference on Learning Representations (ICLR), 2022 </h4>
		    </div>
		  </div>
		  
		  <div class="row">
		  			    <div class="col-lg">
		    <center><h2><strong>
	<a href="https://openreview.net/pdf?id=NkZq4OEYN-">Paper</a> |
	<a href="https://github.com/yyf17/SAAVN/tree/main">Code</a> |
	<a href="./files/ICLR2022_SAAVN_v3.pdf">Slides</a> |
    <a href="./files/bib.txt">Bibtex</a> </strong> </h2></center> 
			</div>
		  </div>
		  <br>
		  
		  <div class="row">
		  	<div class="col-lg">
		  	<img src="./files/saavn.png" class="img-fluid" style="height:215px;">
		  	</div>
		  </div>

		  <br><br>
		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Abstract </h2>
		  	</div>
		  </div>
		  <div class="row">
		  	<div style="font-size:16px"><p align="justify">
			Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. 
				Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most 
				real-world applications due to the unexpected sound noise or intentional interference. In this work, we design an acoustically complex environment in which, 
				besides the target sound, there exists a sound attacker playing a zero-sum game with the agent. More specifically, the attacker can move and change the volume 
				and category of the sound to make the agent suffer from finding the sounding object while the agent tries to dodge the attack and navigate to the goal under 
				the intervention. Under certain constraints to the attacker, we can improve the robustness of the agent towards unexpected sound attacks in audio-visual 
				navigation. For better convergence, we develop a joint training mechanism by employing the property of a centralized critic with decentralized actors. 
				Experiments on two real-world 3D scan datasets, Replica, and Matterport3D, verify the effectiveness and the robustness of the agent trained under our designed 
				environment when transferred to the clean environment or the one containing sound attackers with random policy. 
				Project:  <a href="https://yyf17.github.io/SAAVN">https://yyf17.github.io/SAAVN</a>.
		  	</p>
		  	</div>
		  </div>

		  <br><br>
		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Materials </h2><p></p>
		  	</div>
		  </div>
		  <div class="row">

		    <div class="col-lg">
		    	<a href="https://openreview.net/pdf?id=NkZq4OEYN-">
          			<img src="./files/1_paper-iclr2022_saavn_final.png" style="height:270px" class="img-fluid" alt="Responsive image">
          		</a>
          	  <br>
		      <a href="https://openreview.net/pdf?id=NkZq4OEYN-"><button type="button" class="btn btn-secondary">Paper</button></a>           
		    </div>

		    <div class="col-lg">
		      <a href="./files/poster-saavn.pdf">
          			<img src="files/poster-saavn.png" style="height:270px" class="img-fluid" alt="Responsive image">
          		</a>
          	  <br>
		      <a href="./files/poster-saavn.pdf"><button type="button" class="btn btn-secondary">Poster</button></a>   
		    </div>
		    

		   	<div class="col-lg">
		      <a href="https://github.com/yyf17/SAAVN/tree/main">
		    		<img src="files/code_thumb.jpeg" style="height:270px" class="img-fluid" alt="Responsive image">
		    	</a>
		      <br>
		      <a href="https://github.com/yyf17/SAAVN/tree/main"><button type="button" class="btn btn-secondary">Code</button></a>  
		    </div> 

		  </div> 

		  <br><br>
		  <div class="row">
		  	<div class="col-lg">
		  		<h2>Presentation</h2>
			    <div class=text-center>
				    
                <iframe width="560" height="315" src="./files/ICLR-saavn-v2-1.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			    
				</div>
		  	</div>
		  </div>

          <br><br>
		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Citation </h2>
		  		    <pre align="left">
					@inproceedings{YinfengICLR2022saavn,
					  author    = {Yinfeng Yu and
						       Wenbing Huang and
						       Fuchun Sun and
						       Changan Chen and
						       Yikai Wang and
						       Xiaohong Liu},
					  title     = {Sound Adversarial Audio-Visual Navigation},
					  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
						       2022, Virtual Event, April 25-29, 2022},
					  publisher = {OpenReview.net},
					  year      = {2022},
					  url       = {https://openreview.net/forum?id=NkZq4OEYN-},
					  timestamp = {Thu, 18 Aug 2022 18:42:35 +0200},
					  biburl    = {https://dblp.org/rec/conf/iclr/Yu00C0L22.bib},
					  bibsource = {dblp computer science bibliography, https://dblp.org}
					}
                    </pre>
			</div>
		  </div>

		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Acknowledgement </h2>
		  		<p align="left">
	                The following projects jointly supported this work: 
			the Sino-German Collaborative Research Project Crossmodal Learning (NSFC 62061136001/DFG TRR169); 
			Beijing Science and Technology Plan Project (No.Z191100008019008); 
			the National Natural Science Foundation of China (No.62006137); 
			Natural Science Project of Scientific Research Plan of Colleges and Universities in Xinjiang (No.XJEDU2021Y003).
            <br><br>
		  	</div>
		  </div>		  

		</div>
	</div>



	<div id="intro" style="width: 100%; padding:50px; text-align:center;">
	        <h1></h1>
	</div>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>
   
  </body>
</html>
