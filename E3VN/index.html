<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">

    <title> E3VN </title>
  </head>
  <body>
  	<div style="text-align:center;">
		<div id="intro" style="width: 100%; padding:40px; text-align:center;">
	        <h1>Echo-Enhanced Embodied Visual Navigation</h1>
        </div>
		<div class="container">
		  <div class="row">
		    <div class="col-sm">
		      <a href="https://yyf17.github.io/"><h5> Yinfeng Yu </h5></a>    
		    </div>
		    <div class="col-sm">
		      <h5> Lele Cao</h5>
		    </div>
		    <div class="col-sm">
		      <h5> Fuchun Sun</h5> 
		    </div>
		    <div class="col-sm">
		      <h5> Chao Yang </h5>
                    </div>
                    <div class="col-sm">
                      <h5> Huicheng Lai </h5>
                    </div>
                    <div class="col-sm">
                      <h5> Wenbing Huang </h5>
                    </div>
		  </div>
		  <p></p>
			
		  <div class="row">
		  	<div class="col-lg">
		   	   <h4>  Tsinghua University (THU) </h4>
		        </div>
		  </div>
			
		  <div class="row">
		  	<div class="col-lg">
		   	<h4> Neural Computation (2023) </h4>
		    </div>
		  </div>
		  
		  <div class="row">
		       <div class="col-lg">
		                   <center><h2><strong>
	                           <a href="https://direct.mit.edu/neco/article-pdf/35/5/958/2079357/neco_a_01579.pdf">Paper(MIT)</a> |
			           <a href="./files/E3VN.pdf">Paper</a> |
                                   <a href="./files/bib.txt">Bibtex</a> </strong> </h2></center> 
			</div>
		  </div>
		  <br>
		  
		  <div class="row">
		  	<div class="col-lg">
		  	<img src="./files/E3VN.png" class="img-fluid" style="height:215px;">
		  	</div>
		  </div>

		  <br><br>
			
		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Abstract </h2>
		  	</div>
		  </div>
		  <div class="row">
		  	<div style="font-size:16px">
				<p align="justify">
					Visual navigation involves a movable robotic agent striving to reach a point goal (target
					location) using vision sensory input. While navigation with ideal visibility has seen
					plenty of success, it becomes challenging in sub-optimal visual conditions like poor
					illumination, where the traditional approaches suffer from severe performance degra-
					dation. We propose E3VN (echo enhanced embodied visual navigation) to effectively
					perceive the surroundings even under poor visibility to mitigate this problem. This is
					made possible by adopting an echoer that actively perceives the environment via audi-
					tory signals. E3VN models the robot agent as playing a cooperative Markov game with
					that echoer. The action policies of robot and echoer are jointly optimized to maximize
					the reward in a two-stream actor-critic architecture. During optimization, the reward is
					also adaptively decomposed into the robot and echoer parts. Our experiments and abla-
					tion studies show that E3VN is consistently effective and robust in point goal navigation
					tasks, especially under non-ideal visibility.
		  		</p>
		  	</div>
		  </div>

		  <br><br>


		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Citation </h2>
		  		    <pre align="left">
				    @article{10.1162/neco_a_01579,
					author = {Yu, Yinfeng and Cao, Lele and Sun, Fuchun and Yang, Chao and Lai, Huicheng and Huang, Wenbing},
					title = {Echo-Enhanced Embodied Visual Navigation},
					journal = {Neural Computation},
					volume = {35},
					number = {5},
					pages = {958-976},
					year = {2023},
					month = {04},
					issn = {0899-7667},
					doi = {10.1162/neco_a_01579},
					url = {https://doi.org/10.1162/neco\_a\_01579},
					eprint = {https://direct.mit.edu/neco/article-pdf/35/5/958/2079357/neco\_a\_01579.pdf},
				    }
                   		 </pre>
			</div>
		  </div>

		  <div class="row">
		  	<div class="col-sm">
		  		<h2> Acknowledgement </h2>
		  		<p align="left">
	                		This work is funded by Sino-German Collaborative Research Project Crossmodal Learning with identification number NSFC62061136001/DFG SFB/TRR169.
            			<br><br>
		  	</div>
		  </div>		  

		</div>
	</div>



	<div id="intro" style="width: 100%; padding:50px; text-align:center;">
	        <h1></h1>
	</div>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>
   
  </body>
</html>
